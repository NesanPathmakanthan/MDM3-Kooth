import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Parameters
n_users = 20 # no of users
n_posts = 5000  # no of posts
n_topics = 4  # no of topics
topics = ["Anxiety", "Depression", "LGBTQ+", "Neurodiversity"]
content_types = ["Article", "Forum", "Article Comment", "Forum Comment"]
base_time = datetime(2024, 1, 1)
community_focus = {0: "Anxiety", 1: "Depression", 2: "LGBTQ+", 3: "Neurodiversity"}

# creating users
users = [f"SU{i}" for i in range(1, n_users + 1)]
# dirichlet distribution assigns 4 probabilities of the post being on each topic for each user 
user_topic_probs = {
    user: np.random.dirichlet(alpha=[1] * n_topics).tolist() for user in users  # Random initial probabilities
}
# highest probability is extracted and user is assiged to community based on that topic
# does not mean the users post will be on this topic as the post is generated based on probabilities 
user_communities = {
    user: np.argmax(probs) for user, probs in user_topic_probs.items()  # Initial community (highest probability)
}
# uses gaussian distribution and asisgns time stamps with peaks at 10am and 8pm
# not that neccessary but makes it more realistic 
hourly_activity = np.array([np.exp(-((hour - 10) ** 2) / (2 * 2 ** 2)) +
                            np.exp(-((hour - 20) ** 2) / (2 * 2 ** 2)) for hour in range(24)])
hourly_activity /= hourly_activity.sum()  # normalise

# creates time stamps 
def generate_timestamps(n_posts):
    timestamps = []
    for _ in range(n_posts):
        day_offset = int(np.random.randint(0, 365))
        hour = int(np.random.choice(range(24), p=hourly_activity))
        minute = int(np.random.randint(0, 60))
        second = int(np.random.randint(0, 60))
        timestamps.append(base_time + timedelta(days=day_offset, hours=hour, minutes=minute, seconds=second))
    return timestamps
timestamps = generate_timestamps(n_posts)

# activity follows exponential scale, few very active, most not very active
# does mean no_posts wont be exactly as its defined but not sure it makes that much difference 
activity_levels = np.random.exponential(scale=1.0, size=n_users)
activity_levels /= activity_levels.sum()  # Normalize
user_post_counts = {user: int(n_posts * activity_levels[idx]) for idx, user in enumerate(users)}

# Drift function for how topic probabilities for each user change over time 
# if they change enough then they are assigned to a new community 
def drift_topic_probs(user_probs, drift_factor=0.02):
    drift = drift_factor * np.random.dirichlet(alpha=[1] * len(user_probs))
    new_probs = user_probs + drift
    return new_probs / new_probs.sum()  # Normalize

data = []
content_tracker = []
individual_user_data = [] 
days = 365
# SIMULATING DATA
content_id = 1
for day in range(days):  
    for user, post_count in user_post_counts.items():
        # Calculate new probabilities based on drift and assign community
        user_topic_probs[user] = drift_topic_probs(user_topic_probs[user])
        current_community = np.argmax(user_topic_probs[user])
        user_communities[user] = current_community
        # Creates array for number of posts over 365 days 
        daily_post_count = np.random.multinomial(post_count, [1 / days] * days)[day]
        for _ in range(daily_post_count):
            # normalising probabilities so community structure is more defined 
            # the more normalised it is the easier it is to detect communities 
            transformed_probs = np.array(user_topic_probs[user]) ** 20  
            transformed_probs /= transformed_probs.sum()  
            # Determine the topic of the content
            topic = np.random.choice(topics, p=transformed_probs)          
            # Check if valid ancestors exist for the topic
            valid_ancestors = [
                c["Content_Created_ID"]
                for c in content_tracker
                if c["Content_Topic"] == topic and c["Content_Type"] in ["Article", "Forum"]
            ]           
            # Decide content type
            if valid_ancestors:  # If ancestors exist, allow comments 
                content_type = np.random.choice(content_types, p=[0.2, 0.2, 0.3, 0.3])
                if content_type in ["Article Comment", "Forum Comment"]:
                    ancestor_content_id = np.random.choice(valid_ancestors)
                else:
                    ancestor_content_id = None
            else:  # If no ancestors exist, default to creating articles or forums because you cant have a comment on nothing
                content_type = np.random.choice(["Article", "Forum"], p=[0.5, 0.5])
                ancestor_content_id = None
            
            # Assign a timestamp
            timestamp = base_time + timedelta(days=day) + timedelta(hours=int(np.random.choice(range(24), p=hourly_activity)))

            # Save content information
            content_data = {
                "Content_Created_ID": content_id,
                "Service_User_ID": user,
                "Content_Type": content_type,
                "Content_Topic": topic,
                "Timestamp": timestamp,
                "Ancestor_Content_ID": ancestor_content_id,
                "Topic_Probs": user_topic_probs[user],  # only for evaluation of model cant be used for model itself 
                "Community_ID": current_community  # ""
            }

            # append to trackers for simulation
            content_tracker.append({
                "Content_Created_ID": content_id,
                "Content_Type": content_type,
                "Service_User_ID": user,
                "Community_ID": current_community,
                "Content_Topic": topic
            })

            data.append(content_data)

            # appending data for one user
            # just for checking if its running properly
            if user == "SU1":  
                individual_user_data.append(content_data)

            content_id += 1

# Create DataFrames
df = pd.DataFrame(data)
individual_user_df = pd.DataFrame(individual_user_data)

# main dataset
file_path_main = "/Users/isobelbridge/Documents/mdm4/simulated_kooth_data.csv"
df.to_csv(file_path_main, index=False)
print(f"Main file saved to: {file_path_main}")

# individual user dataset
file_path_individual = "/Users/isobelbridge/Documents/mdm4/individual_user_data.csv"
individual_user_df.to_csv(file_path_individual, index=False)
print(f"Individual user file saved to: {file_path_individual}")
